[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Welcome to my home page, I’m a Data Science/Analytics specialist with more than a decade of work in many IT domains."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Rafael Belokurows",
    "section": "",
    "text": "&lt;p&gt;&lt;/iframe&lt;/p&gt;\n&lt;section id=\"section\" class=\"level2\"&gt;\n&lt;h2&gt;&lt;/h2&gt;\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Rafael Belokurows&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Rafael Belokurows&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Projects\"&gt;Projects&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/projects.html\"&gt;/projects.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Resume\"&gt;Resume&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/resume.html\"&gt;/resume.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Blog\"&gt;Blog&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/blog.html\"&gt;/blog.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Data Viz\"&gt;Data Viz&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/data-viz.html\"&gt;/data-viz.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://github.com/rafabelokurows\"&gt;https://github.com/rafabelokurows&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:https://www.linkedin.com/in/rafabelo\"&gt;https://www.linkedin.com/in/rafabelo&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"hidden\" data-render-id=\"footer-left\"&gt;\n&lt;p&gt;Made with &lt;a href=\"https://quarto.org/\"&gt;Quarto&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"hidden\" data-render-id=\"footer-center\"&gt;\n&lt;p&gt;&lt;a href=\"mailto:rafabelokurows@gmail.com\"&gt;Rafael Belokurows&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"hidden\" data-render-id=\"footer-right\"&gt;\n&lt;p&gt;&lt;a href=\"https://www.github.com/rafabelokurows/\"&gt;Github&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Rafael Belokurows&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Rafael Belokurows&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Rafael Belokurows&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Rafael Belokurows&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const disableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'prefetch';\n    }\n  }\n  const enableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'stylesheet';\n    }\n  }\n  const manageTransitions = (selector, allowTransitions) =&gt; {\n    const els = window.document.querySelectorAll(selector);\n    for (let i=0; i &lt; els.length; i++) {\n      const el = els[i];\n      if (allowTransitions) {\n        el.classList.remove('notransition');\n      } else {\n        el.classList.add('notransition');\n      }\n    }\n  }\n  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) =&gt; {\n    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';\n    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';\n    let newTheme = '';\n    if(darkModeDefault) {\n      newTheme = isAlternate ? baseTheme : alternateTheme;\n    } else {\n      newTheme = isAlternate ? alternateTheme : baseTheme;\n    }\n    const changeGiscusTheme = () =&gt; {\n      // From: https://github.com/giscus/giscus/issues/336\n      const sendMessage = (message) =&gt; {\n        const iframe = document.querySelector('iframe.giscus-frame');\n        if (!iframe) return;\n        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');\n      }\n      sendMessage({\n        setConfig: {\n          theme: newTheme\n        }\n      });\n    }\n    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;\n    if (isGiscussLoaded) {\n      changeGiscusTheme();\n    }\n  }\n  const toggleColorMode = (alternate) =&gt; {\n    // Switch the stylesheets\n    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');\n    manageTransitions('#quarto-margin-sidebar .nav-link', false);\n    if (alternate) {\n      enableStylesheet(alternateStylesheets);\n      for (const sheetNode of alternateStylesheets) {\n        if (sheetNode.id === \"quarto-bootstrap\") {\n          toggleBodyColorMode(sheetNode);\n        }\n      }\n    } else {\n      disableStylesheet(alternateStylesheets);\n      toggleBodyColorPrimary();\n    }\n    manageTransitions('#quarto-margin-sidebar .nav-link', true);\n    // Switch the toggles\n    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');\n    for (let i=0; i &lt; toggles.length; i++) {\n      const toggle = toggles[i];\n      if (toggle) {\n        if (alternate) {\n          toggle.classList.add(\"alternate\");     \n        } else {\n          toggle.classList.remove(\"alternate\");\n        }\n      }\n    }\n    // Hack to workaround the fact that safari doesn't\n    // properly recolor the scrollbar when toggling (#1455)\n    if (navigator.userAgent.indexOf('Safari') &gt; 0 && navigator.userAgent.indexOf('Chrome') == -1) {\n      manageTransitions(\"body\", false);\n      window.scrollTo(0, 1);\n      setTimeout(() =&gt; {\n        window.scrollTo(0, 0);\n        manageTransitions(\"body\", true);\n      }, 40);  \n    }\n  }\n  const isFileUrl = () =&gt; { \n    return window.location.protocol === 'file:';\n  }\n  const hasAlternateSentinel = () =&gt; {  \n    let styleSentinel = getColorSchemeSentinel();\n    if (styleSentinel !== null) {\n      return styleSentinel === \"alternate\";\n    } else {\n      return false;\n    }\n  }\n  const setStyleSentinel = (alternate) =&gt; {\n    const value = alternate ? \"alternate\" : \"default\";\n    if (!isFileUrl()) {\n      window.localStorage.setItem(\"quarto-color-scheme\", value);\n    } else {\n      localAlternateSentinel = value;\n    }\n  }\n  const getColorSchemeSentinel = () =&gt; {\n    if (!isFileUrl()) {\n      const storageValue = window.localStorage.getItem(\"quarto-color-scheme\");\n      return storageValue != null ? storageValue : localAlternateSentinel;\n    } else {\n      return localAlternateSentinel;\n    }\n  }\n  const darkModeDefault = false;\n  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';\n  // Dark / light mode switch\n  window.quartoToggleColorScheme = () =&gt; {\n    // Read the current dark / light value \n    let toAlternate = !hasAlternateSentinel();\n    toggleColorMode(toAlternate);\n    setStyleSentinel(toAlternate);\n    toggleGiscusIfUsed(toAlternate, darkModeDefault);\n  };\n  // Ensure there is a toggle, if there isn't float one in the top right\n  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {\n    const a = window.document.createElement('a');\n    a.classList.add('top-right');\n    a.classList.add('quarto-color-scheme-toggle');\n    a.href = \"\";\n    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };\n    const i = window.document.createElement(\"i\");\n    i.classList.add('bi');\n    a.appendChild(i);\n    window.document.body.appendChild(a);\n  }\n  // Switch to dark mode if need be\n  if (hasAlternateSentinel()) {\n    toggleColorMode(true);\n  } else {\n    toggleColorMode(false);\n  }\n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n    var localhostRegex = new RegExp(/^(?:http|https):\\/\\/localhost\\:?[0-9]*\\//);\n    var mailtoRegex = new RegExp(/^mailto:/);\n      var filterRegex = new RegExp('/' + window.location.host + '/');\n    var isInternal = (href) =&gt; {\n        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);\n    }\n    // Inspect non-navigation links and adorn them if external\n \tvar links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');\n    for (var i=0; i&lt;links.length; i++) {\n      const link = links[i];\n      if (!isInternal(link.href)) {\n        // undo the damage that might have been done by quarto-nav.js in the case of\n        // links that we want to consider external\n        if (link.dataset.originalHref !== undefined) {\n          link.href = link.dataset.originalHref;\n        }\n      }\n    }\n  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {\n    const config = {\n      allowHTML: true,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start',\n    };\n    if (contentFn) {\n      config.content = contentFn;\n    }\n    if (onTriggerFn) {\n      config.onTrigger = onTriggerFn;\n    }\n    if (onUntriggerFn) {\n      config.onUntrigger = onUntriggerFn;\n    }\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      if (note) {\n        return note.innerHTML;\n      } else {\n        return \"\";\n      }\n    });\n  }\n  const xrefs = window.document.querySelectorAll('a.quarto-xref');\n  const processXRef = (id, note) =&gt; {\n    // Strip column container classes\n    const stripColumnClz = (el) =&gt; {\n      el.classList.remove(\"page-full\", \"page-columns\");\n      if (el.children) {\n        for (const child of el.children) {\n          stripColumnClz(child);\n        }\n      }\n    }\n    stripColumnClz(note)\n    if (id === null || id.startsWith('sec-')) {\n      // Special case sections, only their first couple elements\n      const container = document.createElement(\"div\");\n      if (note.children && note.children.length &gt; 2) {\n        container.appendChild(note.children[0].cloneNode(true));\n        for (let i = 1; i &lt; note.children.length; i++) {\n          const child = note.children[i];\n          if (child.tagName === \"P\" && child.innerText === \"\") {\n            continue;\n          } else {\n            container.appendChild(child.cloneNode(true));\n            break;\n          }\n        }\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(container);\n        }\n        return container.innerHTML\n      } else {\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(note);\n        }\n        return note.innerHTML;\n      }\n    } else {\n      // Remove any anchor links if they are present\n      const anchorLink = note.querySelector('a.anchorjs-link');\n      if (anchorLink) {\n        anchorLink.remove();\n      }\n      if (window.Quarto?.typesetMath) {\n        window.Quarto.typesetMath(note);\n      }\n      // TODO in 1.5, we should make sure this works without a callout special case\n      if (note.classList.contains(\"callout\")) {\n        return note.outerHTML;\n      } else {\n        return note.innerHTML;\n      }\n    }\n  }\n  for (var i=0; i&lt;xrefs.length; i++) {\n    const xref = xrefs[i];\n    tippyHover(xref, undefined, function(instance) {\n      instance.disable();\n      let url = xref.getAttribute('href');\n      let hash = undefined; \n      if (url.startsWith('#')) {\n        hash = url;\n      } else {\n        try { hash = new URL(url).hash; } catch {}\n      }\n      if (hash) {\n        const id = hash.replace(/^#\\/?/, \"\");\n        const note = window.document.getElementById(id);\n        if (note !== null) {\n          try {\n            const html = processXRef(id, note.cloneNode(true));\n            instance.setContent(html);\n          } finally {\n            instance.enable();\n            instance.show();\n          }\n        } else {\n          // See if we can fetch this\n          fetch(url.split('#')[0])\n          .then(res =&gt; res.text())\n          .then(html =&gt; {\n            const parser = new DOMParser();\n            const htmlDoc = parser.parseFromString(html, \"text/html\");\n            const note = htmlDoc.getElementById(id);\n            if (note !== null) {\n              const html = processXRef(id, note);\n              instance.setContent(html);\n            } \n          }).finally(() =&gt; {\n            instance.enable();\n            instance.show();\n          });\n        }\n      } else {\n        // See if we can fetch a full url (with no hash to target)\n        // This is a special case and we should probably do some content thinning / targeting\n        fetch(url)\n        .then(res =&gt; res.text())\n        .then(html =&gt; {\n          const parser = new DOMParser();\n          const htmlDoc = parser.parseFromString(html, \"text/html\");\n          const note = htmlDoc.querySelector('main.content');\n          if (note !== null) {\n            // This should only happen for chapter cross references\n            // (since there is no id in the URL)\n            // remove the first header\n            if (note.children.length &gt; 0 && note.children[0].tagName === \"HEADER\") {\n              note.children[0].remove();\n            }\n            const html = processXRef(null, note);\n            instance.setContent(html);\n          } \n        }).finally(() =&gt; {\n          instance.enable();\n          instance.show();\n        });\n      }\n    }, function(instance) {\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            div.style.left = 0;\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n        // Handle positioning of the toggle\n    window.addEventListener(\n      \"resize\",\n      throttle(() =&gt; {\n        elRect = undefined;\n        if (selectedAnnoteEl) {\n          selectCodeLines(selectedAnnoteEl);\n        }\n      }, 10)\n    );\n    function throttle(fn, ms) {\n    let throttle = false;\n    let timer;\n      return (...args) =&gt; {\n        if(!throttle) { // first call gets through\n            fn.apply(this, args);\n            throttle = true;\n        } else { // all the others get throttled\n            if(timer) clearTimeout(timer); // cancel #2\n            timer = setTimeout(() =&gt; {\n              fn.apply(this, args);\n              timer = throttle = false;\n            }, ms);\n        }\n      };\n    }\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;script src=\"https://utteranc.es/client.js\"\n  repo=\"rafabelokurows/rafabelokurows.github.io\"\n  issue-term=\"pathname\"\n  theme=\"github-light\"\n  crossorigin=\"anonymous\"\n  async&gt;\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n&lt;footer class=\"footer\"&gt;\n  &lt;div class=\"nav-footer\"&gt;\n    &lt;div class=\"nav-footer-left\"&gt;\n      &lt;div class='footer-contents'&gt;Made with [Quarto](https://quarto.org/)\n&lt;/div&gt;  \n    &lt;/div&gt;   \n    &lt;div class=\"nav-footer-center\"&gt;\n      &lt;div class='footer-contents'&gt;[Rafael Belokurows](mailto:rafabelokurows@gmail.com)\n&lt;/div&gt;  \n    &lt;/div&gt;\n    &lt;div class=\"nav-footer-right\"&gt;\n      &lt;div class='footer-contents'&gt;[Github](https://www.github.com/rafabelokurows/)\n&lt;/div&gt;  \n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "projects/welcome/index.html",
    "href": "projects/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "projects/post-with-code/index.html",
    "href": "projects/post-with-code/index.html",
    "title": "Primeiro projeto",
    "section": "",
    "text": "This is a post with executable code.\n\nCode1 + 1\n\n[1] 2"
  },
  {
    "objectID": "data-viz.html",
    "href": "data-viz.html",
    "title": "Data Visualizations",
    "section": "",
    "text": "Time Series Anomaly Detection\n\n\n\n\n\nProphet Forecast\n\n\n\n\n\nInflation animation\n\n\n\n\n\nTop European Airlines\n\n\n\n\n\nGender pay gap in Eruopean countries\n\n\n\n\n\nInflation comparison 2022 x 2023"
  },
  {
    "objectID": "blog_posts/post-with-code/index.html",
    "href": "blog_posts/post-with-code/index.html",
    "title": "Churn prediction project",
    "section": "",
    "text": "This post is under construction\n\n\nCode\nimport pandas as pd\n\ndata = {\n  \"calories\": [420, 380, 390],\n  \"duration\": [50, 40, 45]\n}\n\n#load data into a DataFrame object:\ndf = pd.DataFrame(data)\n\nprint(df);\n\n\n   calories  duration\n0       420        50\n1       380        40\n2       390        45"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Rafael Belokurows",
    "section": "",
    "text": "Using Causal Inference to quantify layoff impact on stock price\n\n\n10 min\n\n\n\nR\n\n\nfinance\n\n\ncausal inference\n\n\nstocks\n\n\n\nCan a major layoff be good for business? As far as stock prices are concerned, Causal Inference says it can\n\n\n\nMay 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWorlwide Weather Data\n\n\n9 min\n\n\n\nR\n\n\nweather\n\n\npackages\n\n\ngeo\n\n\ndata\n\n\n\nObtain daily weather data for weather stations all over the world with a few lines of code + some plots to get you started\n\n\n\nMay 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nR tip #1: Don’t lose track of your outputs with ‘.Last.value’\n\n\n2 min\n\n\n\ntips\n\n\nR\n\n\nprogramming\n\n\nRStudio\n\n\n\nUse .Last.value on R to keep data you forgot to save to a variable\n\n\n\nMay 7, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog_posts/welcome/index.html",
    "href": "blog_posts/welcome/index.html",
    "title": "R tip #1: Don’t lose track of your outputs with ‘.Last.value’",
    "section": "",
    "text": "Greetings!\nA quick R tip to get thins going:\nIf you’re a bit distracted like me and you sometimes forget to store your variables or outputs, having to run the same function more than once, like so:\nCodedata.frame(a=c(1,2,3),b=c(4,5,6)) #oops, forgot to save it to a variable\n\n  a b\n1 1 4\n2 2 5\n3 3 6\nInstead of doing this :\nCodedf &lt;- data.frame(a=c(1,2,3),b=c(4,5,6)) #there you go\ndf\n\n  a b\n1 1 4\n2 2 5\n3 3 6\nThen I might have the solution for you. Base R has a function called .Last.value1 that recalls and lets you reuse your last console output (be it whatever it is).\nThis way, you can store it in a variable and do whatever you wanted to do with it in the first place!\nYou may ask me: Ok, but how will that save me time? Can’t I press the up arrow ⬆ and run the same code again? Yeah, sure, but what if you just ran a complex operation on a dataframe with many thousands of rows (or more)?\nUsing .Last.value lets you reuse the same data, this way you won’t waste time or computing power (which in a hosted environment could cost you some money).\nThis is the first of a series of tips to come, stay tuned for more!"
  },
  {
    "objectID": "data_viz/post-with-code/index.html",
    "href": "data_viz/post-with-code/index.html",
    "title": "Primeiro post blog",
    "section": "",
    "text": "This is a post with executable code.\n\nCode1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects/post_with_plot/index.html",
    "href": "projects/post_with_plot/index.html",
    "title": "Segundo projeto",
    "section": "",
    "text": "This is a post with executable code.\n\nCodeplot(mtcars$mpg)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Rafael Belokurows",
    "section": "",
    "text": "Streamlit app that allows the user to load their own time series CSV file and that gives a quick overview of time series features, perform statistical checks and produce short-term forecasts.\nLive Web App\nPowered by: Python, Streamlit, Prophet, time series libraries"
  },
  {
    "objectID": "projects.html#time-series-explorer-web-app",
    "href": "projects.html#time-series-explorer-web-app",
    "title": "Rafael Belokurows",
    "section": "",
    "text": "Streamlit app that allows the user to load their own time series CSV file and that gives a quick overview of time series features, perform statistical checks and produce short-term forecasts.\nLive Web App\nPowered by: Python, Streamlit, Prophet, time series libraries"
  },
  {
    "objectID": "projects.html#porto-street-names-gender-gap",
    "href": "projects.html#porto-street-names-gender-gap",
    "title": "Rafael Belokurows",
    "section": "Porto Street Names Gender Gap",
    "text": "Porto Street Names Gender Gap\n\n\n\n\n\n\nMost cities have a long-standing gender bias towards men when naming streets, squares, gardens, and other public places. While there is a somewhat recent trend to include more women when defining city names, most street names are well-established and hard to change. With this in mind, I analyzed how bad is the gender bias where I live: Porto, Portugal.\nGithub Repo\nPowered by: R, leaflet, openstreetmaps data"
  },
  {
    "objectID": "projects.html#flight-explorer",
    "href": "projects.html#flight-explorer",
    "title": "Rafael Belokurows",
    "section": "Flight Explorer",
    "text": "Flight Explorer\n\n\nAutomated data pipeline that collects daily prices on plane tickets and send e-mail alerts whenever price drops below certain threshold in routes of interest.\nLive Web app\nMade using: Python, API requests, Github Actions, Google Big Query"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rafael Belokurows",
    "section": "",
    "text": "Welcome to my webpage. Here you’ll find a bit about me, read some blog posts (that you’ll hopefully like), check tips and tricks on Data Analysis and Data Science tools, and see some of the stuff I’ve been working on lately. On the professional side, I’m currently working in Analytics/Data Science at Kantar Worldpanel Portugal. In real life, I enjoy reading fiction, biking and traveling.\nTo see some examples of my work and personal projects, check out:\nResume\nData projects\nBlog\nData visualizations"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Rafael Belokurows",
    "section": "",
    "text": "Welcome to my webpage. Here you’ll find a bit about me, read some blog posts (that you’ll hopefully like), check tips and tricks on Data Analysis and Data Science tools, and see some of the stuff I’ve been working on lately. On the professional side, I’m currently working in Analytics/Data Science at Kantar Worldpanel Portugal. In real life, I enjoy reading fiction, biking and traveling.\nTo see some examples of my work and personal projects, check out:\nResume\nData projects\nBlog\nData visualizations"
  },
  {
    "objectID": "index.html#areas-of-interest",
    "href": "index.html#areas-of-interest",
    "title": "Rafael Belokurows",
    "section": "Areas of interest",
    "text": "Areas of interest\n🛒 Consumer trends, Churn prevention, Customer Segmentation\n📊 Product usability metrics, A/B tests\n💹 Time Series Analysis and Forecasting\n🌎 Geographical Models, Transport and Urban Planning\n🌲 Environment, Climate Change, and Sustainability\n Web scraping and process automation\n🏈 Football, Baseball, Soccer"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Rafael Belokurows",
    "section": "Experience",
    "text": "Experience\nKantar Worldpanel | Analytics/Data Science | Apr 2022 - Now"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rafael Belokurows",
    "section": "Education",
    "text": "Education\nUniversidade do Porto | Porto, Portugal Msc in Data Science | Sep 2021"
  },
  {
    "objectID": "data-viz - Copy.html",
    "href": "data-viz - Copy.html",
    "title": "Data Visualizations",
    "section": "",
    "text": "#TidyTuesday is a weekly data project aimed at the R ecosystem. This project was borne out of the R4DS Online Learning Community and the R for Data Science textbook. Emphasis is placed on understanding how to summarize and arrange data to make meaningful charts with ggplot2, tidyr, dplyr, and other tools in the tidyverse ecosystem. However, any code-based methodology is welcome.\nWhile I regularly participate in weekly Tidy Tuesday, these posts highlight my favorite submissions and the corresponding code.\n\n\n\n\n\n\nTip\n\n\n\nYou can find my github code repository for Tidy Tuesday submissions here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrimeiro post blog\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\nRafael Belokurows\n\n\nMar 29, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#flight-explorer-1",
    "href": "projects.html#flight-explorer-1",
    "title": "Rafael Belokurows",
    "section": "Flight Explorer",
    "text": "Flight Explorer\n\n\n\n\nFully automated Quarto report of Portuguese Economy and Tourism indicators. Programed on Git Hub Actions and hosted on GitHub Pages, it automatically retrieves data from Portugues official statistics sources and compare latest data with historical results to give the user a quick overview of the situation of the economy and tourism in Portugal. Live Website\nMade with: R, leaflet, openstreetmaps data"
  },
  {
    "objectID": "projects.html#portugal-automated-kpi-report",
    "href": "projects.html#portugal-automated-kpi-report",
    "title": "Rafael Belokurows",
    "section": "Portugal automated KPI Report",
    "text": "Portugal automated KPI Report\n\n\n\n\n\n\nFully automated Quarto report of Portuguese Economy and Tourism indicators. Programed on Git Hub Actions and hosted on GitHub Pages, it automatically retrieves data from Portugues official statistics sources and compare latest data with historical results to give the user a quick overview of the situation of the economy and tourism in Portugal.\nPowered by: Python, Quarto, Github Actions, GitHub Pages\nLive Website\nGithub Repo"
  },
  {
    "objectID": "blog_posts/welcome/index.html#footnotes",
    "href": "blog_posts/welcome/index.html#footnotes",
    "title": "R tip #1: Don’t lose track of your outputs with ‘.Last.value’",
    "section": "Footnotes",
    "text": "Footnotes\n\nFor more (but not whole more of a lot) on this function, there’s always R’s documentation.↩︎"
  },
  {
    "objectID": "blog_posts/r_tip_2/r_tip_2.html",
    "href": "blog_posts/r_tip_2/r_tip_2.html",
    "title": "R Tip #2",
    "section": "",
    "text": "Greetings!\nA quick R tip to get thins going:\nIf you’re a bit distracted like me and you sometimes forget to store your variables or outputs, having to run the same function more than once, like so:\nCodedata.frame(a=c(1,2,3),b=c(4,5,6)) #oops, forgot to save it to a variable\n\n  a b\n1 1 4\n2 2 5\n3 3 6\nInstead of doing this :\nCodedf &lt;- data.frame(a=c(1,2,3),b=c(4,5,6)) #there you go\ndf\n\n  a b\n1 1 4\n2 2 5\n3 3 6\nThen I might have the solution for you. Base R has a function called .Last.value1 that recalls and lets you reuse your last console output (be it whatever it is).\nThis way, you can store it in a variable and do whatever you wanted to do with it in the first place!\nYou may ask me: Ok, but how will that save me time? Can’t I press the up arrow ⬆ and run the same code again? Yeah, sure, but what if you just ran a complex operation on a dataframe with many thousands of rows (or more)?\nUsing .Last.value lets you reuse the same data, this way you won’t waste time or computing power (which in a hosted environment could cost you some money).\nThis is the first of a series of tips to come, stay tuned for more!"
  },
  {
    "objectID": "blog_posts/r_tip_2/r_tip_2.html#footnotes",
    "href": "blog_posts/r_tip_2/r_tip_2.html#footnotes",
    "title": "R Tip #2",
    "section": "Footnotes",
    "text": "Footnotes\n\nFor more (but not whole more of a lot) on this function, there’s always R’s documentation.↩︎"
  },
  {
    "objectID": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html",
    "href": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html",
    "title": "Obtaining data from tables in PDF files",
    "section": "",
    "text": "Hey, data people!\nI think this one will speak directly to the heart of everyone working with/in Data: You know how PDF files can be at the same time your salvation or your doom? Sometimes, you’ll find the exact piece of information you needed in a PDF on the far reaches of the wide web. On the other hand, a PDF file can also leave you hanging when you needed it the most when you simply cannot convert one table within a PDF file into an usable format fast enough.\nIn this post, you’ll see how to scrape data effectively and swiftly from a PDF with and without tables."
  },
  {
    "objectID": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html#footnotes",
    "href": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html#footnotes",
    "title": "Obtaining data from tables in PDF files",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://github.com/tesseract-ocr/tesseract↩︎"
  },
  {
    "objectID": "blog_posts/r_tip_2/weather_data_r.html",
    "href": "blog_posts/r_tip_2/weather_data_r.html",
    "title": "Worlwide Weather Data",
    "section": "",
    "text": "Hello, Data people!\nIn this post, I’ll show you a great source of daily weather data for your ML projects or personal needs.\nI’m talking about R package GSODR which facilitates obtaining data from the NOAA’s Global Summary of the Day (GSOD). The GSOD is a summary of daily weather conditions based on underlying hourly data points measured at more than 9,000 global weather stations.1 Check out NOAA’s webpage fore more details on it.\nSome of the weather features available include, for each day:\nFor an exhaustive list of all the features available, check here."
  },
  {
    "objectID": "blog_posts/r_tip_2/weather_data_r.html#footnotes",
    "href": "blog_posts/r_tip_2/weather_data_r.html#footnotes",
    "title": "Worlwide Weather Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516↩︎\nhttps://dcl-wrangle.stanford.edu/pivot-advanced.html↩︎"
  },
  {
    "objectID": "blog_posts/r_tip_2/weather_data_r.html#obtaining-the-data",
    "href": "blog_posts/r_tip_2/weather_data_r.html#obtaining-the-data",
    "title": "Worlwide Weather Data",
    "section": "Obtaining the data",
    "text": "Obtaining the data\nGSODR is available on CRAN, so installing it is as easy as install.packages(\"GSODR\").\nLet’s find if a particular place has some weather stations near by. First, when you install the package, it stores a list of stations in a local database, from there, you can look up a country or a place (station name).\n\nCodelibrary(GSODR)\nload(system.file(\"extdata\", \"isd_history.rda\", package = \"GSODR\"))\nisd_history %&gt;% filter(COUNTRY_NAME == \"CANADA\") %&gt;% select(STNID,NAME,LAT,LON,COUNTRY_NAME) %&gt;% head() %&gt;%  gt()\n\n\n\n\n\n\nSTNID\nNAME\nLAT\nLON\nCOUNTRY_NAME\n\n\n\n693840-99999\nBOW DRILL\n44.000\n-59.333\nCANADA\n\n\n693850-99999\nGLOMAR HIGH ISLAND\n47.167\n-62.833\nCANADA\n\n\n693860-99999\nDRURY CREEK\n62.200\n-134.383\nCANADA\n\n\n693870-99999\nCARMACKS\n62.117\n-136.183\nCANADA\n\n\n693880-99999\nKLONDIKE\n64.450\n-138.217\nCANADA\n\n\n693900-99999\nSHELDON LAKE\n62.617\n-131.267\nCANADA\n\n\n\n\n\n\n\nTIP: It’s a good practice to run GSODR::update_station_list() every once in a while to force-update the list of weather stations available.\nIt’s pretty easy to plot an interactive map of the weather stations that we just identified:\n\nCodeleaflet(data = isd_history %&gt;% filter(COUNTRY_NAME == \"CANADA\") %&gt;% \n          filter(!grepl(\"GRATES COVE\",NAME)) %&gt;% sample_n(100)) %&gt;% addTiles() %&gt;%\n  addMarkers(~LON, ~LAT, popup = ~as.character(NAME), label = ~as.character(NAME))\n\n\n\n\n\nObs: I’m sampling randomly only 100 weather stations so the map is lighter, but there are around 2K stations in Canada alone. 🤯\nAnother important function of this package is nearest_stations, which allows to find the nearest stations based on the geographical coordinates you provide. For instance, when looking up Toronto’s Downtown coordinates, we see a few stations nearby:\n\nCode(nearby_stations = nearest_stations(LAT = 43.653,LON = -79.384,\n                                    distance = 10) %&gt;% \n  select(STNID,NAME,LAT,LON,COUNTRY_NAME,distance_km)) %&gt;% gt()\n\n\n\n\n\n\nSTNID\nNAME\nLAT\nLON\nCOUNTRY_NAME\ndistance_km\n\n\n\n715080-99999\nTORONTO CITY ONT\n43.667\n-79.400\nCANADA\n2.0\n\n\n712654-99999\nTORONTO ISL (MARS)\n43.633\n-79.400\nCANADA\n2.6\n\n\n726247-99999\nTORONTO IL ARPT AUT\n43.633\n-79.400\nCANADA\n2.6\n\n\n712650-99999\nTORONTO CITY CENTRE\n43.617\n-79.383\nCANADA\n4.0\n\n\n\n\n\n\nCodeleaflet(data = nearby_stations) %&gt;% addTiles() %&gt;%\n  addMarkers(~LON, ~LAT, popup = ~as.character(NAME), label = ~as.character(NAME))\n\n\n\n\n\nWhen you’re finished choosing one or multiple stations, the next step is obtaining the actual data for them:\n\nCode(weather_data = get_GSOD(years = c(2020:2024), station = \"712650-99999\") %&gt;% \n   select(STNID,NAME,MONTH,DAY,YEAR,TEMP,MAX,MIN,RH, I_FOG ,SLP,WDSP,PRCP,DATE = YEARMODA)) %&gt;% tail(5) %&gt;% gt()\n\n\n\n\n\n\nSTNID\nNAME\nMONTH\nDAY\nYEAR\nTEMP\nMAX\nMIN\nRH\nI_FOG\nSLP\nWDSP\nPRCP\nDATE\n\n\n\n712650-99999\nTORONTO CITY CENTRE\n5\n8\n2024\n14.7\n21.6\n7.1\n65.5\n1\n1001.8\n7.5\n5.08\n2024-05-08\n\n\n712650-99999\nTORONTO CITY CENTRE\n5\n9\n2024\n12.3\n21.6\n9.2\n71.5\n0\n1009.9\n3.1\n0.00\n2024-05-09\n\n\n712650-99999\nTORONTO CITY CENTRE\n5\n10\n2024\n13.2\n14.6\n9.2\n59.2\n0\n1009.4\n3.6\n0.00\n2024-05-10\n\n\n712650-99999\nTORONTO CITY CENTRE\n5\n11\n2024\n10.9\n13.0\n8.0\n80.1\n1\n1007.4\n3.0\n0.00\n2024-05-11\n\n\n712650-99999\nTORONTO CITY CENTRE\n5\n12\n2024\n11.9\n17.5\n8.2\n81.3\n1\n1008.5\n4.4\n1.78\n2024-05-12\n\n\n\n\n\n\n\nTIP: You can also get data for multiple years and/or multiple stations at once."
  },
  {
    "objectID": "blog_posts/r_tip_2/weather_data_r.html#plots",
    "href": "blog_posts/r_tip_2/weather_data_r.html#plots",
    "title": "Worlwide Weather Data",
    "section": "Plots",
    "text": "Plots\nHaving gotten the data, there are tons of interesting things to plot. I’ll show you some examples, feel free to get inspire and create even better looking plots :)\nExample 1: Temperature as lines\nHere, I’m plotting Maximum (red) and Minimum temperatures each day for over 2 years of Toronto weather data, which we’ve obtained in the previous step.\nThere is also quite some customization going on, such as:\n\nSmooth line for the two series (minimum and maximum)\nChanging size of fonts\nDisabling legend\nChanging color palette to use colors widely identifiable as “cold” and “heat”\n\nTIP: As many other ggplot2 situations, it’s a good practice to transform your data to “long format” using pivot_longer to transform it to name/value pairs.2\n\nCodeggplot(temperatures, aes(x=DATE, y=value, color=temp, linetype=temp)) + \n  geom_point() + \n  scale_colour_brewer(palette=\"Set1\") +\n  geom_smooth(span = 0.1) +\n  ggtitle(\"Toronto City Centre daily Min. and Max. Temperatures (2022-2024)\") +   \n  theme(plot.title=element_text(size=18, face=\"bold\"),\n        axis.title.y=element_text(size=15),legend.position = \"None\",\n        legend.text=element_text(size=13),\n        axis.title.x=element_text(size=16)) + \n  labs(x =NULL, y= expression(paste(\"Temperature (\",degree,\"C)\")))+theme_few() #scale_y_continuous(limits=c(-30, 50)) + scale_x_continuous(limits=c(0, 200)) \n\n\n\n\n\n\n\nExample 2: Rainy Days By Season\nWith this plot, I wanted to see the difference in the number of rainy days from one season to another. It should be interesting also to compare this aspect for different regions that have dry/wet seasons in different parts of the year. The first roadblock here was: how to define what is a season based on the dates, since weather seasons change in different days of the month and you also have to break months in parts. For that purpose, I’ve borrowed a function I had seen on Stack Overflow and made some concessions to get to a quicker solution setting season change dates always on the 21th, for the sake of simplicity. 😆\n\nShow the codeweather_data$rained = ifelse(weather_data$PRCP == 0 | is.na(weather_data$PRCP),\"No Rain\",\"Rain\")\n\n#Based on the code obtained from:\n#https://stackoverflow.com/questions/9500114/find-which-season-a-particular-date-belongs-to\ntoSeason &lt;- function(dat) {\n  stopifnot(class(dat) == \"Date\")\n  scalarCheck &lt;- function(dat) {\n    m &lt;- as.POSIXlt(dat)$mon+ 1        # correct for 0:11 range\n    d &lt;- as.POSIXlt(dat)$mday           # correct for 0:11 range\n    if ((m == 3 & d &gt;= 21) | (m == 4) | (m == 5) | (m == 6 & d &lt; 21)) {\n      r &lt;- 1\n    } else if ((m == 6 & d &gt;= 21) | (m == 7) | (m == 8) | (m == 9 & d &lt; 21)) {\n      r &lt;- 2\n    } else if ((m == 9 & d &gt;= 21) | (m == 10) | (m == 11) | (m == 12 & d &lt; 21)) {\n      r &lt;- 3\n    } else {\n      r &lt;- 4\n    }\n    r\n  }\n  res &lt;- sapply(dat, scalarCheck)\n  res &lt;- ordered(res, labels=c(\"Spring\", \"Summer\", \"Fall\", \"Winter\"))\n  invisible(res)\n}\nweather_data$season = toSeason(weather_data$DATE)\n\n\nHaving done that, we can then plot a 100% stacked bar chart of each Season and on how many days it rained for each one. As you can see for this particular case, Season were more homogeneous in terms of rain in Toronto in 2023.\n\nCodeweather_data %&gt;% \n  filter(YEAR %in% c(2022,2023)) %&gt;%\n  ggplot(aes(x = season,fill=rained)) +\n  geom_bar( position=\"fill\") +\n  stat_count(geom = \"text\", \n             aes(label = ..count..),\n             position=position_fill(vjust=0.5), colour=\"black\")+\n  scale_fill_manual(values = c(\"#EDEFBD\",\"#ACD1E9\"))+\n  ggtitle(\"Rainy Days in Toronto, by Season (2022 and 2023)\") +\n  labs(fill = NULL, x=NULL, y=\"% of days\")+\n  scale_y_continuous(labels = scales::percent_format())+\n  facet_grid(~YEAR)+theme_few()\n\n\n\n\n\n\n\nExample 3: Horizon Plots to visualize temperature changes over the years\nOkay, I can’t remember where I’ve seen this one, but I thought it was an ingenious way of plotting something like daily temperatures and I wanted to replicate it with my own data. Since it scales from the lowest to the highest temperature present in the data, it allows to see variations in temperature across many years. As you will probably agree, it seems that 2023 in Toronto had milder temperatures both in the Winter and in the Summer with much less prominent spikes over the entire year.\nCredits to ggHoriPlot’s vignette which had some great examples that I borrowed from.\n\nCodelibrary(ggHoriPlot) \ncutpoints = weather_data %&gt;% \n  mutate(\n    outlier = between(\n      TEMP, \n      quantile(weather_data$TEMP, 0.25, na.rm=T)-\n        1.5*IQR(weather_data$TEMP, na.rm=T),\n      quantile(weather_data$TEMP, 0.75, na.rm=T)+\n        1.5*IQR(weather_data$TEMP, na.rm=T))) %&gt;% \n  filter(outlier)\n\nori &lt;- sum(range(cutpoints$TEMP))/2\nsca &lt;- seq(range(cutpoints$TEMP)[1], \n           range(cutpoints$TEMP)[2], \n           length.out = 7)[-4]\n\n\n\nCodeweather_data %&gt;% \n  filter(YEAR &lt;2024) %&gt;% \n  mutate(date_mine = as.Date(str_glue(\"2024-{MONTH}-{DAY}\"))) %&gt;%  #constant date, just to assemble one year on top of the other\n  ggplot() +\n  geom_horizon(aes(date_mine, \n                   TEMP,\n                   fill = ..Cutpoints..), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(YEAR~.) +\n  theme_few() +\n  scale_x_date(expand=c(0,0), \n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  xlab(NULL) +\n  theme(\n    panel.spacing.y=unit(0, \"lines\"),\n    strip.text.y = element_text(size = 16, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank(),\n    legend.position = \"None\",\n    plot.title=element_text(size=20, face=\"bold\"),\n    axis.text.x = element_text(size=16,hjust=-1)\n  ) +\n  labs(title ='Daily Temperature in Toronto, Canada',x=NULL,subtitle=\"The stronger the color, the more extreme the temperature\")"
  },
  {
    "objectID": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html#how-to-get-data-from-a-table-in-your-pdf-file",
    "href": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html#how-to-get-data-from-a-table-in-your-pdf-file",
    "title": "Obtaining data from tables in PDF files",
    "section": "How to get data from a table in your PDF file",
    "text": "How to get data from a table in your PDF file\nIn order to obtain data from a tabular PDF file, you will use R package tabulizer, a fantastic tool for getting metadata and data from a PDF file. To be honest, it can be a bit tricky to install this one, though. Whenever you have to mess with Java and/or environment variables, you just know things can get messy real quick, but these are the prerequisites:\n\nJava Development Kit (JDK) installed\nEnvironment variable JAVA_HOME set to the JDK folder\nR Packages “remotes” and “rJava” installed\nRTools installed\n\nThen, you can finally execute this command to install the package:\nremotes::install_github(c(\"ropensci/tabulizerjars\", \"ropensci/tabulizer\"), INSTALL_opts = \"--no-multiarch\")\nHere is the step by step guide on how to install in much more detail if you have any difficulties:\n\nHaving done all that, now on to the good stuff:\nAnd one of the best options\nThis is the first of a series of tips to come, stay tuned for more!"
  },
  {
    "objectID": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html#simpler-text",
    "href": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html#simpler-text",
    "title": "Obtaining data from tables in PDF files",
    "section": "Simple(r) text",
    "text": "Simple(r) text\nAs a first try you almost never need to deploy the biggest and more comprehensive tool; maybe the simpler and quicker method is the one that gets the job done, right? So why don’t start with a couple of simple yet very effective methods.\n1. For digital files with straightforward text - pdf_text() from pdftools\n\nWhen your PDF file contains entirely (or mostly) flowing text, it’s easier to begin with\n\nCodelibrary(pdftools)\ndownload.file(\"http://arxiv.org/pdf/1403.2805.pdf\", \"1403.2805.pdf\", mode = \"wb\")\ntxt &lt;- pdf_text(\"1403.2805.pdf\")\n\n# first page text\ncat(txt[1])\n\n                                              The jsonlite Package: A Practical and Consistent Mapping\n                                                                   Between JSON Data and R Objects\n\n                                                                                    Jeroen Ooms\narXiv:1403.2805v1 [stat.CO] 12 Mar 2014\n\n                                                                              UCLA Department of Statistics\n\n                                                                                             Abstract\n                                                  A naive realization of JSON data in R maps JSON arrays to an unnamed list, and JSON objects to a\n                                               named list. However, in practice a list is an awkward, inefficient type to store and manipulate data.\n                                               Most statistical applications work with (homogeneous) vectors, matrices or data frames. Therefore JSON\n                                               packages in R typically define certain special cases of JSON structures which map to simpler R types.\n                                               Currently there exist no formal guidelines, or even consensus between implementations on how R data\n                                               should be represented in JSON. Furthermore, upon closer inspection, even the most basic data structures\n                                               in R actually do not perfectly map to their JSON counterparts and leave some ambiguity for edge cases.\n                                               These problems have resulted in different behavior between implementations and can lead to unexpected\n                                               output. This paper explicitly describes a mapping between R classes and JSON data, highlights potential\n                                               problems, and proposes conventions that generalize the mapping to cover all common structures. We\n                                               emphasize the importance of type consistency when using JSON to exchange dynamic data, and illustrate\n                                               using examples and anecdotes. The jsonlite R package is used throughout the paper as a reference\n                                               implementation.\n\n                                          1    Introduction\n\n                                          JavaScript Object Notation (JSON) is a text format for the serialization of structured data (Crockford, 2006a).\n                                          It is derived from the object literals of JavaScript, as defined in the ECMAScript Programming Language\n                                          Standard, Third Edition (ECMA, 1999). Design of JSON is simple and concise in comparison with other\n                                          text based formats, and it was originally proposed by Douglas Crockford as a “fat-free alternative to XML”\n                                          (Crockford, 2006b). The syntax is easy for humans to read and write, easy for machines to parse and generate\n                                          and completely described in a single page at http://www.json.org. The character encoding of JSON text\n                                          is always Unicode, using UTF-8 by default (Crockford, 2006a), making it naturally compatible with non-\n                                          latin alphabets. Over the past years, JSON has become hugely popular on the internet as a general purpose\n                                          data interchange format. High quality parsing libraries are available for almost any programming language,\n                                          making it easy to implement systems and applications that exchange data over the network using JSON. For\n                                          R (R Core Team, 2013), several packages that assist the user in generating, parsing and validating JSON\n                                          are available through CRAN, including rjson (Couture-Beil, 2013), RJSONIO (Lang, 2013), and jsonlite\n                                          (Ooms et al., 2014).\n\n                                          The emphasis of this paper is not on discussing the JSON format or any particular implementation for using\n\n                                                                                                 1\n\nCode# second page text\ncat(txt[2])\n\nJSON with R. We refer to Nolan and Temple Lang (2014) for a comprehensive introduction, or one of the\nmany tutorials available on the web. Instead we take a high level view and discuss how R data structures are\nmost naturally represented in JSON. This is not a trivial problem, particulary for complex or relational data\nas they frequently appear in statistical applications. Several R packages implement toJSON and fromJSON\nfunctions which directly convert R objects into JSON and vice versa. However, the exact mapping between\nthe various R data classes JSON structures is not self evident. Currently, there are no formal guidelines,\nor even consensus between implementations on how R data should be represented in JSON. Furthermore,\nupon closer inspection, even the most basic data structures in R actually do not perfectly map to their\nJSON counterparts, and leave some ambiguity for edge cases. These problems have resulted in different\nbehavior between implementations, and can lead to unexpected output for certain special cases. To further\ncomplicate things, best practices of representing data in JSON have been established outside the R community.\nIncorporating these conventions where possible is important to maximize interoperability.\n\n\n1.1    Parsing and type safety\n\nThe JSON format specifies 4 primitive types (string, number, boolean, null) and two universal structures:\n\n   • A JSON object : an unordered collection of zero or more name/value pairs, where a name is a string and\n     a value is a string, number, boolean, null, object, or array.\n\n   • A JSON array: an ordered sequence of zero or more values.\n\nBoth these structures are heterogeneous; i.e. they are allowed to contain elements of different types. There-\nfore, the native R realization of these structures is a named list for JSON objects, and unnamed list for\nJSON arrays. However, in practice a list is an awkward, inefficient type to store and manipulate data in R.\nMost statistical applications work with (homogeneous) vectors, matrices or data frames. In order to give\nthese data structures a JSON representation, we can define certain special cases of JSON structures which get\nparsed into other, more specific R types. For example, one convention which all current implementations\nhave in common is that a homogeneous array of primitives gets parsed into an atomic vector instead of a\nlist. The RJSONIO documentation uses the term “simplify” for this, and we adopt this jargon.\ntxt &lt;- \"[12, 3, 7]\"\nx &lt;- fromJSON(txt)\nis(x)\n\n[1] \"numeric\" \"vector\"\n\nprint(x)\n\n[1] 12   3   7\n\nThis seems very reasonable and it is the only practical solution to represent vectors in JSON. However the\nprice we pay is that automatic simplification can compromise type-safety in the context of dynamic data.\nFor example, suppose an R package uses fromJSON to pull data from a JSON API on the web, similar to\nthe example above. However, for some particular combination of parameters, the result includes a null\nvalue, e.g: [12, null, 7]. This is actually quite common, many APIs use null for missing values or unset\nfields. This case makes the behavior of parsers ambiguous, because the JSON array is technically no longer\n\n                                                     2\n\n\nYou can see pdf_text already does a great job at identifying text and outputting into an indexed format. Even without any kind of post-processing, which is something you’ll definitely need in real-world cases, you can sometimes obtain something very clean like the output above, stored in a list with every item being one page of the file you’ve just OCR’d.\n2. scanned text - ocr() from tesseract\n\nOCR tools have evolved pretty well in the last years, especially with the evolving popularity of tesseract, an OCR engine with interfaces in Python, R and many other programming languages.1\nYou don’t even need to an image format, you can read in directly from a PDF file to try to perform the OCR:\n\nCodelibrary(tesseract)\n#ocr_result = ocr()"
  },
  {
    "objectID": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html#tables",
    "href": "blog_posts/scraping_pdf_tables/scraping_pdf_tables.html#tables",
    "title": "Obtaining data from tables in PDF files",
    "section": "Tables",
    "text": "Tables"
  },
  {
    "objectID": "blog_posts/r_tip_2_/r_tip_2_streaks.html",
    "href": "blog_posts/r_tip_2_/r_tip_2_streaks.html",
    "title": "R tip #2: Working with streaks in R",
    "section": "",
    "text": "Hey, data people!\nSomething I’ve come across quite a few times when working with different types of data is detecting patterns of consecutive behavior. This has many applications in practice, such as to determine:\n\nString of occurrences of binomially-distributed phenomenon, such as coin toss, but also basically anything that has Yes/No, Pass/Fail, 1/0 possible results\n\n\nHow many weeks a row a user is using a feature of your website/product\nHitting streaks in Baseball or streaks of 3-point shoots in Basketball\n\nFor some cases, you also may want just the opposite: finding breaks in consecutive behavior, and that follows basically the same logic with just a little tweaks.\nI’ll like to think that everyone working in Data should have one “get-things-done-quickly tool”, be it Python, SQL, R, or Excel if that’s your thing. Some tool you can depend on when you need something done as quickly and effectively as possible. For me, that’s R and the tidyverse, so that’s what I’m working with here.\nLet’s see how to determine and label a streak of values using R and the tidyverse suite of packages.\nWe’re gonna work with\nHere’s the full code for this particular solution. If you know what you’re doing, you can just copy and paste this and change it to your heart’s desire. And don’t worry, if you want some more insight, I’ll explain each part in more detail.\n\nCode# df %&gt;% mutate(lagged = dplyr::lag(corte_cestas)) %&gt;% \n#       mutate(start = (corte_cestas != lagged)) %&gt;% mutate(start = if_else(is.na(start),T,start))%&gt;%\n#       mutate(streak_id = cumsum(start)) %&gt;%\n#       group_by(streak_id) %&gt;% mutate(streak = row_number()) %&gt;% ungroup() %&gt;%\n#       mutate(streak_cestas = if_else(corte_cestas ==0,0,streak)) %&gt;% select(-c(lagged,start,streak_id,streak))\n\n\nand also connect everything at the end of the post, so stay tuned for the full code.\nFirst, we find the lag of the value for the current row.\n\nCode# df_with_lag = df %&gt;% \n#   mutate(lagged = dplyr::lag(value)) \n\n\n\nCode# is_value_lag_equal = df_with_lag %&gt;%\n#   mutate(start = (value != lagged)) \n\n\n\nCode# fixed_start = is_value_lag_equal %&gt;% \n#   mutate(start = if_else(is.na(start),T,start)) \n\n\n\nCode# fixed_start\n#   mutate(streak_id = cumsum(start)) %&gt;%\n#   group_by(streak_id) %&gt;% \n#   mutate(streak = row_number()) %&gt;% \n#   ungroup() %&gt;%\n#   mutate(streak_cestas = if_else(corte_cestas ==0,0,streak)) %&gt;% \n#   select(-c(lagged,start,streak_id,streak))\n\n\n\nYou may ask me: Ok, but how will that save me time? Can’t I press the up arrow ⬆ and run the same code again? Yeah, sure, but what if you just ran a complex operation on a dataframe with many thousands of rows (or more)?\nUsing .Last.value lets you reuse the same data, this way you won’t waste time or computing power (which in a hosted environment could cost you some money).\nThis is the first of a series of tips to come, stay tuned for more!"
  },
  {
    "objectID": "blog_posts/investors_love_layoffs/index.html",
    "href": "blog_posts/investors_love_layoffs/index.html",
    "title": "Using Causality to assess layoff impact on stock value",
    "section": "",
    "text": "Hello, there.\nI’m gonna tackle a topic that is somewhat new to me, but in an area which I have a lot of curiosity to break into, and it’s Finance. In fact, I would appreciate any feedback on this if you think I’m talking nonsense 😊\nHere’s how I got there: I was watching TV, minding my own business, when I saw on some Finance news channel that some company had had a major layoff - and that sucks for the people involved, I hope everyone got a good severance package out of that. But then, it surprised me a bit to hear that the company actually had had a bump in the stock price of XX%, don’t remember how much exactly.\nWhile admittedly a bit surprised that a layoff as big as that would have such a positive impact on a company’s value, I wanted to test this hypothesis with another concrete case, a massive layoff of thousands of people at SAP, one of the world’s leading companies in business management and supply chain software. Check here for more details on that particular piece of news.\nNOTE: There are some good resources online on why this happens, and fundamentally, it boils down to the optics, i.e. the perception that investors and the market overall have on the company, and how “reestructuring” shows promise of improvement in the company’s situation. 12"
  },
  {
    "objectID": "blog_posts/investors_love_layoffs/index.html#footnotes",
    "href": "blog_posts/investors_love_layoffs/index.html#footnotes",
    "title": "Using Causality to assess layoff impact on stock value",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://clutejournals.com/index.php/JABR/article/download/2195/2172/8714↩︎\nhttps://money.com/tech-layoffs-affect-stock-prices/↩︎"
  },
  {
    "objectID": "blog_posts/investors_love_layoffs/index.html#using-causality-to-assess-layoff-impact",
    "href": "blog_posts/investors_love_layoffs/index.html#using-causality-to-assess-layoff-impact",
    "title": "Using Causality to assess layoff impact on stock value",
    "section": "Using Causality to assess layoff impact",
    "text": "Using Causality to assess layoff impact\nFirst, I obtained daily trading data for SAP stocks on the New York Stock Exchange, symbol named SAP SE. Here, I used tidyquant, a great package for R programming language that has several reliable functions to streamline download and analysis of Financial data.\n\nCodelibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(gt)\nstocks = c(\"SAP\") %&gt;%\n  tq_get(get = \"stock.prices\", from = \"2022-01-01\", to = Sys.Date())\n\n\nWhile there are some possible spots where we could analyze ups or downs, we are concerned with the particular movement on Jan 23rd and after, as highlighted in the plot below.\n\nCodelibrary(ggplot2)\nstocks %&gt;%\n    ggplot(aes(x = as.Date(date), y = adjusted)) +\n    geom_line() +\n    labs(title = \"SAP Stock price\", y = \"Closing (adjusted) Price\", x = \"\",subtitle = \"Possible effect of layoffs on price\") + \n    theme_tq() +\n    geom_vline(xintercept = c(as.Date(\"2024-01-23\")),\n                linetype=4, colour=\"black\")\n\n\n\n\n\n\n\nNext, I’ve applied Causal Effect estimation methods to measure and quantify the Causal Effect of this intervention, i.e. the layoff, or to be more precise the day where the decision to layoff became public - 23rd Jan 24, on the response variable, the SAP stock prices after the date of intervention.\nFor that, I’ve used CausalImpact, package developed by the good folks at Google. CausalImpact is an R package for causal inference using Bayesian structural time-series models.\nGiven a response time series stock prices and a set of control time series, the package constructs a Bayesian structural time-series model. This model is then used to try and predict the counterfactual, i.e., how the response metric would have evolved after the intervention if the intervention had never occurred.\nAfter transforming the data and setting the dates for the pre-intervention and post-intervention period, we run CausalImpact’s method for estimation.\n\nCodesap_stock_data = stocks %&gt;% \n  select(date,adjusted)%&gt;%\n  rename(y=adjusted)\ntail(sap_stock_data) %&gt;% gt()\n\n\n\n\n\n\ndate\ny\n\n\n\n2024-05-16\n190.67\n\n\n2024-05-17\n192.80\n\n\n2024-05-20\n195.30\n\n\n2024-05-21\n194.72\n\n\n2024-05-22\n193.91\n\n\n2024-05-23\n195.02\n\n\n\n\n\n\nCodepre.period &lt;- as.Date(c((\"2022-01-03\") ,(\"2024-01-23\")))\npost.period &lt;- as.Date(c((\"2024-01-24\"),(\"2024-05-17\")))\n\n\nAnd we can already see some promising results, with only the own stock time series to feed into the Causal Impact estimation. Some things to unpack here:\n\nThe first gray vertical line shows the date of intervention (layoffs).\nThe horizontal blue dashed line on top is the counterfactual, i.e. what the stock price would be IF the layoffs hadn’t happened, according to our estimation.\nThe squiggly blue dashed line below means the daily difference between the actual stock price in the real world x our counterfactual estimation, and the amount ranges from close to 0 to almost 40USD depending on the day.\n\n\nCodeimpact &lt;- CausalImpact(sap_stock_data, pre.period, post.period)\nplot(impact, c(\"original\", \"pointwise\")) +\n  labs(title = \"Causal Effect layoffs on SAP stock price - first try\")+\n  coord_cartesian(xlim = c(as.Date(\"2022-06-01\"),\n                           as.Date(\"2024-05-17\"))) +\n  scale_x_date(labels = scales::label_date(format = \"%Y %b\")) +\n  theme_bw(base_family = \"Bricolage Grotesque\")\n\n\n\n\n\n\n\nYou can also easily get the coefficients and Average Treatment Effect (or in this case, Average effect of the layoffs) of this estimation, and while the posterior probability of a causal effect looks great (99.5%), I’m not loving the wide confidence interval for the effect (3,4% to 27%), I think we could do better than that with some more data.\n\nCodeimpact \n\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative    \nActual                   183           14841         \nPrediction (s.d.)        162 (8.5)     13082 (684.6) \n95% CI                   [144, 177]    [11703, 14368]\n                                                     \nAbsolute effect (s.d.)   22 (8.5)      1759 (684.6)  \n95% CI                   [5.8, 39]     [472.6, 3138] \n                                                     \nRelative effect (s.d.)   14% (6.1%)    14% (6.1%)    \n95% CI                   [3.3%, 27%]   [3.3%, 27%]   \n\nPosterior tail-area probability p:   0.00451\nPosterior prob. of a causal effect:  99.54904%\n\nFor more details, type: summary(impact, \"report\")"
  },
  {
    "objectID": "blog_posts/investors_love_layoffs/index.html#improving-the-estimation",
    "href": "blog_posts/investors_love_layoffs/index.html#improving-the-estimation",
    "title": "Using Causality to assess layoff impact on stock value",
    "section": "Improving the estimation",
    "text": "Improving the estimation\nLooking to improve the results, one option is to add covariates to the causal estimation. Good covariates should mainly:\n\nBe highly correlated with the response time series\nNot have been affected by the intervention\n\nDirect competitor stocks wouldn’t work here, because they could probably be positively affected when SAP’s stock price goes down or go down when SAP value is on the rise.\nWith that in mind, I’ve obtained stock prices for several other software companies to find out which could make for good covariates to help improve the confidence in our results. And here they didn’t have to be software or IT companies, I just felt like it was easier going for some companies in the same sector. Don’t worry, we’ll exclude direct competitors if they come up as the top correlated stocks after these.\nthe Technology sector, more notably, of companies in the software sector.\n\nCodelibrary(rvest)\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nCodeurl = \"https://stockanalysis.com/stocks/industry/software-application/\"\npage = read_html(url)\nsoftware_stocks = page %&gt;% html_elements(\".symbol-table\") %&gt;% html_table() %&gt;%   .[[1]]\nsoftware_stocks %&gt;% head %&gt;% gt()\n\n\n\n\n\n\nNo.\nSymbol\nCompany Name\nMarket Cap\n% Change\nVolume\nRevenue\n\n\n\n1\nCRM\nSalesforce, Inc.\n263.43B\n-2.75%\n5,178,353\n34.86B\n\n\n2\nSAP\nSAP SE\n228.21B\n0.19%\n465,214\n34.50B\n\n\n3\nINTU\nIntuit Inc.\n170.71B\n-8.05%\n2,760,756\n15.81B\n\n\n4\nNOW\nServiceNow, Inc.\n152.18B\n-2.22%\n710,080\n9.48B\n\n\n5\nUBER\nUber Technologies, Inc.\n134.26B\n0.94%\n5,734,406\n38.59B\n\n\n6\nCDNS\nCadence Design Systems, Inc.\n80.68B\n0.53%\n509,631\n4.08B\n\n\n\n\n\n\n\n\nCode# teste = stocks %&gt;% filter(symbol %in% c(\"SAP\",\"PTC\",\"DUOL\",\"APPF\",\"WK\")) %&gt;% select(date,symbol,adjusted) %&gt;%\n#   pivot_wider(values_from=adjusted,names_from=symbol) %&gt;%\n#   rename(y=SAP,x1=PTC,x2=DUOL,x3=APPF,x4=WK) %&gt;% filter(!is.na(x2)) %&gt;%\n#   select(date,y,x1,x2)\n# pre.period &lt;- as.Date(c((\"2021-07-28\") ,(\"2024-01-23\")))\n# post.period &lt;- c(as.Date(\"2024-01-24\"),as.Date(\"2024-05-17\"))\n\n\nYou may ask me: Ok, but how will that save me time? Can’t I press the up arrow ⬆ and run the same code again? Yeah, sure, but what if you just ran a complex operation on a dataframe with many thousands of rows (or more)?\nUsing .Last.value lets you reuse the same data, this way you won’t waste time or computing power (which in a hosted environment could cost you some money).\nThis is the first of a series of tips to come, stay tuned for more!"
  },
  {
    "objectID": "blog_posts/causality_layoff_impact/index.html",
    "href": "blog_posts/causality_layoff_impact/index.html",
    "title": "Using Causal Inference to quantify layoff impact on stock price",
    "section": "",
    "text": "Hello, there.\nThis week I’m sharing a few new methods I’ve tried recently to connect two fields I’m very interested in: Finance and Causal Inference.\nSAP is a huge world-class all-powerful software company, and even world-class all-powerful companies go through rough patches and reestructuring, as it has happened with SAP over the past few years. Part of this process included a major layoff of 6.000 people in January 2024.\nThe market responded quite positively, with their stock price on the NY Stock Exchange going up a few percent over the following weeks (and it hasn’t gone down since). Was the layoff causally related with the uptick in stock price? Or was it just a coincidence?\nIn this post I’m gonna show how I used Causal Inference estimation methods to identify and measure the impact of this particular layoff on their stock price.\nIf you are somewhat surprised that a big event of layoff would have such a positive impact on a company’s value, there are some good resources online on why this happens, and fundamentally, it boils down to the optics, i.e. the perception that investors and the market overall have on the company, and how “reestructuring” shows promise of improvement in the company’s situation. 12\nData and code are available at the github repository: https://github.com/rafabelokurows/layoffs_stock_price"
  },
  {
    "objectID": "blog_posts/causality_layoff_impact/index.html#using-causality-to-assess-layoff-impact",
    "href": "blog_posts/causality_layoff_impact/index.html#using-causality-to-assess-layoff-impact",
    "title": "Using Causality to assess layoff impact on stock value",
    "section": "Using Causality to assess layoff impact",
    "text": "Using Causality to assess layoff impact\nFirst, I obtained daily trading data for SAP stocks on the New York Stock Exchange, symbol named SAP SE. Here, I used tidyquant, a great package for R programming language that has several reliable functions to streamline download and analysis of Financial data.\n\nCodelibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(gt)\nstocks = c(\"SAP\") %&gt;%\n  tq_get(get = \"stock.prices\", from = \"2022-01-01\", to = Sys.Date())\n\n\nWhile there are some possible spots where we could analyze ups or downs, we are concerned with the particular movement on Jan 23rd and after, as highlighted in the plot below.\n\nCodelibrary(ggplot2)\nstocks %&gt;%\n    ggplot(aes(x = as.Date(date), y = adjusted)) +\n    geom_line() +\n    labs(title = \"SAP Stock price\", y = \"Closing (adjusted) Price\", x = \"\",subtitle = \"Possible effect of layoffs on price\") + \n    theme_tq() +\n    geom_vline(xintercept = c(as.Date(\"2024-01-23\")),\n                linetype=4, colour=\"black\")\n\n\n\n\n\n\n\nNext, I’ve applied Causal Effect estimation methods to measure and quantify the Causal Effect of this intervention, i.e. the layoff, or to be more precise the day where the decision to layoff became public - 23rd Jan 24, on the response variable, the SAP stock prices after the date of intervention.\nFor that, I’ve used CausalImpact, package developed by the good folks at Google. CausalImpact is an R package for causal inference using Bayesian structural time-series models.\nGiven a response time series stock prices and a set of control time series, the package constructs a Bayesian structural time-series model. This model is then used to try and predict the counterfactual, i.e., how the response metric would have evolved after the intervention if the intervention had never occurred.\nAfter transforming the data and setting the dates for the pre-intervention and post-intervention period, we run CausalImpact’s method for estimation.\n\nCodesap_stock_data = stocks %&gt;% \n  select(date,adjusted)%&gt;%\n  rename(y=adjusted)\ntail(sap_stock_data) %&gt;% gt()\n\n\n\n\n\n\ndate\ny\n\n\n\n2024-05-16\n190.67\n\n\n2024-05-17\n192.80\n\n\n2024-05-20\n195.30\n\n\n2024-05-21\n194.72\n\n\n2024-05-22\n193.91\n\n\n2024-05-23\n195.02\n\n\n\n\n\n\nCodepre.period &lt;- as.Date(c((\"2022-01-03\") ,(\"2024-01-23\")))\npost.period &lt;- as.Date(c((\"2024-01-24\"),(\"2024-05-17\")))\n\n\nAnd we can already see some promising results, with only the own stock time series to feed into the Causal Impact estimation. Some things to unpack here:\n\nThe first gray vertical line shows the date of intervention (layoffs).\nThe horizontal blue dashed line on top is the counterfactual, i.e. what the stock price would be IF the layoffs hadn’t happened, according to our estimation.\nThe squiggly blue dashed line below means the daily difference between the actual stock price in the real world x our counterfactual estimation, and the amount ranges from close to 0 to almost 40USD depending on the day.\n\n\nCodeimpact &lt;- CausalImpact(sap_stock_data, pre.period, post.period)\nplot(impact, c(\"original\", \"pointwise\")) +\n  labs(title = \"Causal Effect layoffs on SAP stock price - first try\")+\n  coord_cartesian(xlim = c(as.Date(\"2022-06-01\"),\n                           as.Date(\"2024-05-17\"))) +\n  scale_x_date(labels = scales::label_date(format = \"%Y %b\")) +\n  theme_bw(base_family = \"Bricolage Grotesque\")\n\n\n\n\n\n\n\nYou can also easily get the coefficients and Average Treatment Effect (or in this case, Average effect of the layoffs) of this estimation, and while the posterior probability of a causal effect looks great (99.5%), I’m not loving the wide confidence interval for the effect (3,4% to 27%), I think we could do better than that with some more data.\n\nCodeimpact \n\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative    \nActual                   183           14841         \nPrediction (s.d.)        162 (8.5)     13082 (684.7) \n95% CI                   [145, 177]    [11711, 14358]\n                                                     \nAbsolute effect (s.d.)   22 (8.5)      1759 (684.7)  \n95% CI                   [6, 39]       [483, 3129]   \n                                                     \nRelative effect (s.d.)   14% (6.1%)    14% (6.1%)    \n95% CI                   [3.4%, 27%]   [3.4%, 27%]   \n\nPosterior tail-area probability p:   0.00451\nPosterior prob. of a causal effect:  99.54904%\n\nFor more details, type: summary(impact, \"report\")"
  },
  {
    "objectID": "blog_posts/causality_layoff_impact/index.html#improving-the-estimation",
    "href": "blog_posts/causality_layoff_impact/index.html#improving-the-estimation",
    "title": "Using Causal Inference to quantify layoff impact on stock price",
    "section": "Improving the estimation",
    "text": "Improving the estimation\nLooking to improve the results, one option is to add covariates to the causal estimation. Good covariates should mainly:\n\nBe highly correlated with the response time series\nNot have been affected by the intervention\n\nDirect competitor stocks wouldn’t work here, because they could probably be positively affected when SAP’s stock price goes down or go down when SAP value is on the rise.\nWith that in mind, I’ve obtained stock prices for several other of companies in the software sector to find out which could make for good covariates to help improve the confidence in our results. And here they didn’t have to be software or IT companies, I just felt like it was easier going for some companies in the same sector. Don’t worry, we’ll exclude direct competitors if they come up as the top correlated stocks after these.\n\nCodeurl = \"https://stockanalysis.com/stocks/industry/software-application/\"\npage = read_html(url)\nsoftware_stocks = page %&gt;% html_elements(\".symbol-table\") %&gt;% html_table() %&gt;%   .[[1]]\nsoftware_stocks %&gt;% head %&gt;% gt()\n\n\n\n\n\n\nNo.\nSymbol\nCompany Name\nMarket Cap\n% Change\nVolume\nRevenue\n\n\n\n1\nCRM\nSalesforce, Inc.\n264.46B\n-2.25%\n8,059,238\n34.86B\n\n\n2\nSAP\nSAP SE\n228.05B\n0.18%\n648,669\n34.52B\n\n\n3\nINTU\nIntuit Inc.\n169.94B\n-8.35%\n4,187,180\n15.81B\n\n\n4\nNOW\nServiceNow, Inc.\n151.39B\n-2.59%\n1,341,199\n9.48B\n\n\n5\nUBER\nUber Technologies, Inc.\n134.27B\n1.04%\n10,337,236\n38.59B\n\n\n6\nCDNS\nCadence Design Systems, Inc.\n80.08B\n-0.14%\n939,947\n4.08B\n\n\n\n\n\n\n\nNOTE_1: As they are ordered by Market Cap (Value), we see Salesforce and SAP at the top, but there are more than 100 companies in the list, so we’re gonna stick with the first 100 for brevity :)\nNOTE_2: Here is the list of software-related stocks we’ve obtained as of May 25th, 2024, just in case.\nNow, we use tidyquant again to obtain data for all 100 stocks. You can access this particular dataset here.\n\nCodestocks = unique(software_stocks$Symbol)[1:100] %&gt;%\n  tq_get(get = \"stock.prices\", from = \"2022-01-03\", to = \"2024-05-23\")\n\nall_stock_data = stocks %&gt;% \n  select(symbol, date,adjusted) %&gt;%\n  filter(!is.na(adjusted))\n\n\nAnd then, we run a matching method available in package MarketMatching (which you can install with devtools::install_github(\"https://github.com/klarsen1/MarketMatching\")\n\nCodemm &lt;- MarketMatching::best_matches(data=all_stock_data,\n                                   id=\"symbol\",\n                                   markets_to_be_matched = c(\"SAP\"),\n                                   date_variable=\"date\",\n                                   matching_variable=\"adjusted\",\n                                   parallel=F,\n                                   start_match_period=\"2022-01-03\",\n                                   end_match_period=\"2024-01-23\",\n                                   matches = 10\n)\nmm$BestMatches %&gt;% \n    filter(symbol == \"SAP\") %&gt;% \n    select(BestControl,rank,RelativeDistance,Correlation,Correlation_of_logs) %&gt;% \n    left_join(software_stocks %&gt;% select(Symbol ,`Company Name`,`Market Cap`,Revenue), by=c(\"BestControl\"=\"Symbol\")) %&gt;% \n    relocate(`Company Name`,.before=rank) %&gt;% \n    rename(Symbol = BestControl) %&gt;% \n  mutate(across(c(RelativeDistance,Correlation,Correlation_of_logs),round,3)) %&gt;% \n  head(5) %&gt;% \n    gt()%&gt;%\n    gt_highlight_rows(\n        rows = c(1:3),\n        fill = \"#ccd5ae\",\n        bold_target_only = TRUE\n    )\n\n\n\n\n\n\nSymbol\nCompany Name\nrank\nRelativeDistance\nCorrelation\nCorrelation_of_logs\nMarket Cap\nRevenue\n\n\n\nPTC\nPTC Inc.\n1\n0.214\n0.861\n0.837\n21.91B\n2.24B\n\n\nAPPF\nAppFolio, Inc.\n2\n0.325\n0.867\n0.871\n8.54B\n671.78M\n\n\nDUOL\nDuolingo, Inc.\n3\n0.352\n0.830\n0.784\n7.68B\n583.00M\n\n\nWK\nWorkiva Inc.\n4\n0.421\n0.658\n0.718\n4.36B\n655.52M\n\n\nMNDY\nmonday.com Ltd.\n5\n0.484\n0.680\n0.704\n11.91B\n784.35M\n\n\n\n\n\n\n\nFunction best_matches, amongst other things, identifies the “markets”, in this case stocks prices, that show the highest similarity with the stock price for SAP. Those are the stocks that would make for the best control groups to use as predictors in the Causal Impact analysis.\nAfter preparing the new dataset including stock prices for SAP and the two most similar stocks (PTC, DUOL and APPF), we run the CausalImpact method again. Another change which also help to improve the results was increasing the number of iterations of the model from the default 1000 to 5000 (beware, the model will take a bit longer to fit).\n\nCodesap_and_covariates = all_stock_data %&gt;% \n  filter(symbol %in% c(\"SAP\",\"PTC\",\"DUOL\",\"APPF\")) %&gt;% \n  select(date,symbol,adjusted) %&gt;%\n  pivot_wider(values_from=adjusted,names_from=symbol) %&gt;%\n  rename(y=SAP,x1=PTC,x2=DUOL) %&gt;% filter(!is.na(x2)) %&gt;%\n  select(date,y,x1,x2)\n\nimpact_2 &lt;- CausalImpact(sap_and_covariates, pre.period, post.period,\n                         model.args = list(niter = 5000))\nplot(impact_2, c(\"original\", \"pointwise\")) +\n  labs(title = \"Causal Effect layoffs on SAP stock price\",y=\"Stock Price (USD)\")+\n  coord_cartesian(xlim = c(as.Date(\"2022-01-01\"),\n                           as.Date(\"2024-05-17\"))) +\n  scale_x_date(labels = scales::label_date(format = \"%Y %b\")) +\n  theme_bw(base_family = \"Bricolage Grotesque\")\n\n\n\n\n\n\n\n\nCodeimpact_2\n\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative    \nActual                   183           14841         \nPrediction (s.d.)        162 (5.4)     13141 (437.3) \n95% CI                   [152, 173]    [12278, 13979]\n                                                     \nAbsolute effect (s.d.)   21 (5.4)      1700 (437.3)  \n95% CI                   [11, 32]      [862, 2562]   \n                                                     \nRelative effect (s.d.)   13% (3.8%)    13% (3.8%)    \n95% CI                   [6.2%, 21%]   [6.2%, 21%]   \n\nPosterior tail-area probability p:   2e-04\nPosterior prob. of a causal effect:  99.97985%\n\nFor more details, type: summary(impact, \"report\")\n\n\nWe get different (higher) estimated effects compared with the previous execution, and a narrower confidence interval: there is 95% confidence that the causal effect lies between 6,1% and 21%, instead of the 3,3% to 27% interval obtained in the first try. That can definitely be further improved, but it’s a good jump in quality from the first model.\nFor sake of brevity we’ll stop here, but also know you can fit your own Bayesian model, or passing additional arguments, include multiple seasonality terms and adjust other parameters of your Bayesian underlying model. Check the official vignette of CausalImpact to see the additional model parameters you can pass to CausalImpact or if you’re a Bayesian aficionado, how to fit your own BSTS model."
  },
  {
    "objectID": "blog_posts/causality_layoff_impact/index.html#footnotes",
    "href": "blog_posts/causality_layoff_impact/index.html#footnotes",
    "title": "Using Causal Inference to quantify layoff impact on stock price",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://clutejournals.com/index.php/JABR/article/download/2195/2172/8714↩︎\nhttps://money.com/tech-layoffs-affect-stock-prices/↩︎"
  },
  {
    "objectID": "blog_posts/causality_layoff_impact/index.html#using-causal-inference-to-assess-layoff-impact-on-stock-price",
    "href": "blog_posts/causality_layoff_impact/index.html#using-causal-inference-to-assess-layoff-impact-on-stock-price",
    "title": "Using Causal Inference to quantify layoff impact on stock price",
    "section": "Using Causal Inference to assess layoff impact on stock price",
    "text": "Using Causal Inference to assess layoff impact on stock price\nFirst, I obtained daily trading data for SAP stocks on the New York Stock Exchange, symbol named SAP SE. Here, I used tidyquant, a great package for R programming language that has several reliable functions to streamline download and analysis of Financial data.\n\nCodelibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(gt)\nlibrary(gtExtras)\nstocks = c(\"SAP\") %&gt;%\n  tq_get(get = \"stock.prices\", from = \"2022-01-01\", to = Sys.Date())\n\n\nWhile there are some possible spots where we could analyze ups or downs, we are concerned with the particular movement on Jan 23rd and after, as highlighted in the plot below.\n\nCodelibrary(ggplot2)\nstocks %&gt;%\n    ggplot(aes(x = as.Date(date), y = adjusted)) +\n    geom_line() +\n    labs(title = \"SAP Stock price\", y = \"Closing (adjusted) Price\", x = \"\",subtitle = \"Possible effect of layoffs on price\") + \n    theme_tq() +\n    geom_vline(xintercept = c(as.Date(\"2024-01-23\")),\n                linetype=4, colour=\"black\")\n\n\n\n\n\n\n\nNext, I’ve applied Causal Effect estimation methods to measure and quantify the Causal Effect of this intervention, i.e. the layoff, or to be more precise the day where the decision to layoff became public - 23rd Jan 24, on the response variable, the SAP stock prices after the date of intervention.\nFor that, I’ve used CausalImpact, package developed by the good folks at Google. CausalImpact is an R package for causal inference using Bayesian structural time-series models.\nGiven a response time series stock prices and a set of control time series, the package constructs a Bayesian structural time-series model. This model is then used to try and predict the counterfactual, i.e., how the response metric would have evolved after the intervention if the intervention had never occurred.\nAfter transforming the data and setting the dates for the pre-intervention and post-intervention period, we run CausalImpact’s method for estimation.\n\nCodesap_stock_data = stocks %&gt;% \n  select(date,adjusted)%&gt;%\n  rename(y=adjusted)\ntail(sap_stock_data) %&gt;% gt()\n\n\n\n\n\n\ndate\ny\n\n\n\n2024-05-17\n192.80\n\n\n2024-05-20\n195.30\n\n\n2024-05-21\n194.72\n\n\n2024-05-22\n193.91\n\n\n2024-05-23\n195.02\n\n\n2024-05-24\n195.38\n\n\n\n\n\n\nCodepre.period &lt;- as.Date(c((\"2022-01-03\") ,(\"2024-01-23\")))\npost.period &lt;- as.Date(c((\"2024-01-24\"),(\"2024-05-17\")))\n\n\nAnd we can already see some promising results, with only the own stock time series to feed into the Causal Impact estimation. Some things to unpack here:\n\nThe first gray vertical line shows the date of intervention (layoffs).\nThe horizontal blue dashed line on top is the counterfactual, i.e. what the stock price would be IF the layoffs hadn’t happened, according to our estimation.\nThe squiggly blue dashed line below means the daily difference between the actual stock price in the real world x our counterfactual estimation, and the amount ranges from close to 0 to almost 40USD depending on the day.\n\n\nCodeimpact &lt;- CausalImpact(sap_stock_data, pre.period, post.period)\nplot(impact, c(\"original\", \"pointwise\")) +\n  labs(title = \"Causal Effect layoffs on SAP stock price - first try\")+\n  coord_cartesian(xlim = c(as.Date(\"2022-06-01\"),\n                           as.Date(\"2024-05-17\"))) +\n  scale_x_date(labels = scales::label_date(format = \"%Y %b\")) +\n  theme_bw(base_family = \"Bricolage Grotesque\")\n\n\n\n\n\n\n\nYou can also easily get the coefficients and Average Treatment Effect (or in this case, Average effect of the layoffs) of this estimation, and while the posterior probability of a causal effect looks great (99.5%), I’m not loving the wide confidence interval for the effect (3,3% to 27%), I think we could do better than that with some more data.\n\nCodeimpact \n\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative    \nActual                   183           14841         \nPrediction (s.d.)        161 (8.8)     13031 (709.1) \n95% CI                   [144, 178]    [11644, 14427]\n                                                     \nAbsolute effect (s.d.)   22 (8.8)      1810 (709.1)  \n95% CI                   [5.1, 39]     [413.3, 3197] \n                                                     \nRelative effect (s.d.)   14% (6.3%)    14% (6.3%)    \n95% CI                   [2.9%, 27%]   [2.9%, 27%]   \n\nPosterior tail-area probability p:   0.00974\nPosterior prob. of a causal effect:  99.02558%\n\nFor more details, type: summary(impact, \"report\")"
  },
  {
    "objectID": "blog_posts/causality_layoff_impact/index.html#takeaways",
    "href": "blog_posts/causality_layoff_impact/index.html#takeaways",
    "title": "Using Causal Inference to quantify layoff impact on stock price",
    "section": "Takeaways",
    "text": "Takeaways\n\nYou can use Causal Inference to assess causal impact of an event on a time series.\nMore data and more iterations of model fitting will help more than hurt you.\nLayoffs are good for business(?), at least the business that are already struggling or going through reestructuring.\n\nAs always, hope you liked, and any feedback is appreciated. Thanks for reading!"
  },
  {
    "objectID": "blog_posts/causality_layoff_impact/index.html#obtaining-the-data-and-first-causal-estimation",
    "href": "blog_posts/causality_layoff_impact/index.html#obtaining-the-data-and-first-causal-estimation",
    "title": "Using Causal Inference to quantify layoff impact on stock price",
    "section": "Obtaining the data and first Causal estimation",
    "text": "Obtaining the data and first Causal estimation\nFirst, I obtained daily trading data for SAP stocks on the New York Stock Exchange, symbol named SAP SE. Here, I used tidyquant, a great package for R programming language that has several reliable functions to streamline download and analysis of Financial data.\nHere’s the corresponding dataset, with data as of May 23rd 2024.\n\nCodesap_stock = c(\"SAP\") %&gt;%\n  tq_get(get = \"stock.prices\", from = \"2022-01-01\", to = \"2024-05-23\")\n\n\nWhile there are some possible spots where we could analyze ups or downs, we are concerned with the particular movement on Jan 23rd and after, as highlighted in the plot below.\n\nCodesap_stock %&gt;%\n    ggplot(aes(x = as.Date(date), y = adjusted)) +\n    geom_line() +\n    labs(title = \"SAP Stock price\", y = \"Closing (adjusted) Price\", x = \"\",subtitle = \"Possible effect of layoffs on price\") + \n    theme_tq() +\n    geom_vline(xintercept = c(as.Date(\"2024-01-23\")),\n                linetype=4, colour=\"black\")\n\n\n\n\n\n\n\nNext, I’ve applied a Causal Effect estimation method to measure and quantify the Causal Effect of this intervention, i.e. the layoff, or to be more precise the day where the decision to layoff became public - 23rd Jan 24, on the response variable, the SAP stock prices after the date of intervention.\nFor that, I’ve used CausalImpact, package developed by the good folks at Google, for causal inference using Bayesian structural time-series models.\nGiven a response time series stock prices and a set of control time series, the package constructs a Bayesian structural time-series model. This model is then used to try and predict the counterfactual, i.e., how the response metric would have evolved after the intervention if the intervention had never occurred.\n\nCodesap_stock_data = sap_stock %&gt;% \n  select(date,adjusted)%&gt;%\n  rename(y=adjusted)\ntail(sap_stock_data) %&gt;% gt()\n\n\n\n\n\n\ndate\ny\n\n\n\n2024-05-15\n190.055\n\n\n2024-05-16\n190.670\n\n\n2024-05-17\n192.800\n\n\n2024-05-20\n195.300\n\n\n2024-05-21\n194.720\n\n\n2024-05-22\n193.910\n\n\n\n\n\n\nCodepre.period &lt;- as.Date(c((\"2022-01-03\") ,(\"2024-01-23\")))\npost.period &lt;- as.Date(c((\"2024-01-24\"),(\"2024-05-17\")))\n\n\nAfter transforming the data and setting the dates for the pre-intervention and post-intervention period, we run CausalImpact’s method for estimation.\n\nCodeimpact &lt;- CausalImpact(sap_stock_data, pre.period, post.period)\nplot(impact, c(\"original\", \"pointwise\")) +\n  labs(title = \"Causal Effect of layoffs on SAP stock price - first try\",y=\"Stock Price (USD)\")+\n  coord_cartesian(xlim = c(as.Date(\"2022-06-01\"),\n                           as.Date(\"2024-05-17\"))) +\n  scale_x_date(labels = scales::label_date(format = \"%Y %b\")) +\n  theme_bw(base_family = \"Bricolage Grotesque\")\n\n\n\n\n\n\n\nAnd we can already see some promising results, with only the own stock time series to feed into the Causal Impact estimation. Some things to unpack here:\n\nThe first gray vertical line shows the date of intervention (layoffs).\nThe horizontal blue dashed line on top is the counterfactual, i.e. what the stock price would be IF the layoffs hadn’t happened, according to our estimation.\nThe squiggly blue dashed line below means the daily difference between the actual stock price in the real world x our counterfactual estimation, and the amount ranges from close to 0 to almost 40USD depending on the day.\n\n\nCodeimpact \n\nPosterior inference {CausalImpact}\n\n                         Average       Cumulative    \nActual                   183           14841         \nPrediction (s.d.)        161 (8.6)     13072 (695.3) \n95% CI                   [144, 178]    [11669, 14425]\n                                                     \nAbsolute effect (s.d.)   22 (8.6)      1768 (695.3)  \n95% CI                   [5.1, 39]     [416.1, 3172] \n                                                     \nRelative effect (s.d.)   14% (6.1%)    14% (6.1%)    \n95% CI                   [2.9%, 27%]   [2.9%, 27%]   \n\nPosterior tail-area probability p:   0.00642\nPosterior prob. of a causal effect:  99.3576%\n\nFor more details, type: summary(impact, \"report\")\n\n\nYou can also easily get the coefficients and Average Treatment Effect (or in this case, Average effect of the layoffs) of this estimation, and while the posterior probability of a causal effect looks great (+99%), I’m not loving the wide confidence interval for the effect (~3% to 27%), I think we could do better than that with some more data."
  },
  {
    "objectID": "blog_posts/causality_layoff_impact/index.html#key-takeaways",
    "href": "blog_posts/causality_layoff_impact/index.html#key-takeaways",
    "title": "Using Causal Inference to quantify layoff impact on stock price",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nThanks to Google’s CausalImpact R package and some correlated stock as predictors, we calculate a posterior probability of causal effect of 99,89%. 😎\nMore data and more iterations of model fitting will give a much needed confidence boost.\nLayoffs are good for business(?), at least the business that are already struggling or going through reestructuring.\n\nAs always, hope you liked, and any feedback is appreciated.\nThanks for reading!"
  }
]